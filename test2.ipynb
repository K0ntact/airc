{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Generator, Tuple, List\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def pad_frame_buffer(frame_buffer: List[np.ndarray], buffer_size: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Pad frame buffer with last frame if needed.\n",
    "\n",
    "    Args:\n",
    "        frame_buffer: List of frames\n",
    "        buffer_size: Desired buffer size\n",
    "\n",
    "    Returns:\n",
    "        Padded frame buffer\n",
    "    \"\"\"\n",
    "    while len(frame_buffer) < buffer_size:\n",
    "        frame_buffer.append(frame_buffer[-1])\n",
    "    return frame_buffer\n",
    "\n",
    "def extract_frame_tensors(\n",
    "    video_path: str,\n",
    "    frames_per_second: int = 8,\n",
    "    buffer_size: int = 16,\n",
    "    frame_stride: int = 8\n",
    ") -> Generator[Tuple[np.ndarray], None, None]:\n",
    "    \"\"\"\n",
    "    Extract frame tensors from video in a memory-efficient way using frame buffers.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        frames_per_second: Number of frames to extract per second\n",
    "        buffer_size: Number of frames in each tensor buffer\n",
    "        frame_stride: Number of frames to stride between buffers\n",
    "\n",
    "    Returns:\n",
    "        Generator yielding tuples of (frame_tensor, timestamp)\n",
    "        frame_tensor shape: (buffer_size, height, width, channels)\n",
    "    \"\"\"\n",
    "    if not Path(video_path).exists():\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Failed to open video file\")\n",
    "\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        duration = total_frames / fps\n",
    "        target_frames = np.linspace(0,\n",
    "                                    total_frames - 1,\n",
    "                                    num=int(duration * frames_per_second),\n",
    "                                    dtype=np.int32)\n",
    "        print(target_frames)\n",
    "        print(len(target_frames))\n",
    "\n",
    "        frame_buffer = []\n",
    "        current_frame = 0\n",
    "\n",
    "        for target_idx in target_frames:\n",
    "            # Skip to target frame\n",
    "            while current_frame < target_idx:\n",
    "                cap.read()\n",
    "                current_frame += 1\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_buffer.append(frame)\n",
    "            current_frame += 1\n",
    "\n",
    "            if len(frame_buffer) == buffer_size:\n",
    "                frame_tensor = np.stack(frame_buffer, axis=0)\n",
    "                frame_tensor = np.transpose(frame_tensor, (0, 3, 1, 2))\n",
    "                yield frame_tensor\n",
    "\n",
    "                # Slide buffer window by stride\n",
    "                frame_buffer = frame_buffer[frame_stride:]\n",
    "\n",
    "        # # Handle remaining frames if any\n",
    "        # if len(frame_buffer) >= buffer_size // 2:\n",
    "        #     frame_buffer = pad_frame_buffer(frame_buffer, buffer_size)\n",
    "        #     frame_tensor = np.stack(frame_buffer, axis=0)\n",
    "        #     frame_tensor = np.transpose(frame_tensor, (0, 3, 1, 2))\n",
    "        #     yield frame_tensor\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "def process_video(video_path: str) -> None:\n",
    "    \"\"\"Example usage of the frame tensor extractor\"\"\"\n",
    "    frame_tensors = extract_frame_tensors(\n",
    "        video_path,\n",
    "        frames_per_second=8,\n",
    "        buffer_size=16,\n",
    "        frame_stride=8\n",
    "    )\n",
    "\n",
    "    for idx, tensor in enumerate(frame_tensors):\n",
    "        print(f\"Frame tensor {idx}: shape {tensor.shape}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc9dd1d023a7c9c4",
   "metadata": {},
   "source": [
    "process_video(\"./datasets/ATMA-V/videos/train/BT-aug/11-aug-gauss_blur.mp4\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T09:30:13.400298Z",
     "start_time": "2025-02-10T09:30:12.194337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset import TimesformerData\n",
    "\n",
    "dataset = TimesformerData(\n",
    "    vid_folder_path=\"./datasets/ATMA-V/videos/train/aug\",\n",
    "    label_path=\"./datasets/ATMA-V/labels/labels.txt\"\n",
    ")"
   ],
   "id": "192a8cbfeed218aa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T09:30:13.434906Z",
     "start_time": "2025-02-10T09:30:13.404383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor, label = dataset[0]\n",
    "tensor.shape, label"
   ],
   "id": "cbffc4e1e8e4e93a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 3, 224, 224]), tensor(0))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vid_tensor_count = {}\n",
    "for tup in dataset.video_tensor_idx_mapping:\n",
    "    if tup[0] not in vid_tensor_count:\n",
    "        vid_tensor_count[tup[0]] = 0\n",
    "    vid_tensor_count[tup[0]] += 1"
   ],
   "id": "92c5bf88e0dd1e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset.video_tensor_idx_mapping",
   "id": "900e1b7f86616c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a01ac327e7f34b9",
   "metadata": {},
   "source": [
    "sorted_vid_tensor_count = dict(sorted(vid_tensor_count.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_vid_tensor_count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecb5fe92943f40cf",
   "metadata": {},
   "source": [
    "stt = \"13\"\n",
    "for tup in dataset.video_tensor_sequence_mapping:\n",
    "    if stt in tup[0]:\n",
    "        print(tup)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a65010f4d5bdea21",
   "metadata": {},
   "source": [
    "import torch\n",
    "vid_tensor_normal_anomaly_count = {}\n",
    "\n",
    "for vid_path, tensor_idx in dataset.video_tensor_idx_mapping:\n",
    "    # vid_path: (normal, anomaly)\n",
    "    if vid_path not in vid_tensor_normal_anomaly_count:\n",
    "        vid_tensor_normal_anomaly_count[vid_path] = (0, 0)\n",
    "    _, label = dataset._load_frame_tensor(vid_path, tensor_idx)\n",
    "    \n",
    "    if torch.equal(label, torch.tensor([1., 0.])):\n",
    "        vid_tensor_normal_anomaly_count[vid_path] = (vid_tensor_normal_anomaly_count[vid_path][0] + 1, vid_tensor_normal_anomaly_count[vid_path][1])\n",
    "    else:\n",
    "        vid_tensor_normal_anomaly_count[vid_path] = (vid_tensor_normal_anomaly_count[vid_path][0], vid_tensor_normal_anomaly_count[vid_path][1] + 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7bd8cf3e68cf8acf",
   "metadata": {},
   "source": [
    "vid_tensor_normal_anomaly_count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25e3989202256083",
   "metadata": {},
   "source": [
    "normal_count = 0\n",
    "anomaly_count = 0\n",
    "for tup in vid_tensor_normal_anomaly_count.values():\n",
    "    normal_count += tup[0]\n",
    "    anomaly_count += tup[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fdf16a0c1a1525f5",
   "metadata": {},
   "source": [
    "normal_count, anomaly_count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e60e72790f4a5b7",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
